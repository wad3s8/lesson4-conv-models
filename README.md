# Отчёт по Домашнему Заданию №3: Полносвязные сети

---

Студент: Утробин Владислав Александрович Среда выполнения: Google Colab, PyTorch с поддержкой CUDA (NVIDIA GPU)

## 1.1 Сравнение моделей разной глубины (15 баллов)

Глубина влияет на форму кривых: сети с 1–2 слоями сходятся медленно и дают низкую точность, в то время как глубже 5 — быстрее выходят на плато и обучаются до 99%.

Вывод:
Повышение глубины до 3–5 слоев улучшает качество.
Однако глубже 5 — прирост на тесте незначителен, а переобучение усиливается.
Линейный классификатор (depth=1) даёт заметно худшие результаты.

## 1.2 Анализ переобучения (15 баллов)

У глубокой сети (7 слоев) точность на трейне продолжает расти, а тестовая застаивается или даже снижается — явный признак переобучения.
У моделей с 2–3 слоями кривые train/test идут близко, расхождения минимальны.
У 5-слойной модели — ещё допустимая разница, но заметен переход в переобучение к 8–10 эпохе.

Оптимальная глубина:
3–5 слоев дают наилучший компромисс между качеством и устойчивостью к переобучению.

Регуляризация:
Добавление Dropout и BatchNorm (в дальнейшем эксперименте) снижает переобучение у глубокой сети:
Dropout(0.3) между скрытыми слоями помогает удерживать обобщающую способность
BatchNorm1d стабилизирует обучение и ускоряет сходимость

## 2.1 Влияние размера ядра свертки (15 баллов)

Сравнение точности и потерь
Наилучшую точность показало ядро 7×7, обеспечив лучшее распознавание объектов.
Комбинация 1×1 + 3×3 также дала хороший результат, демонстрируя потенциал модульного подхода.

Рецептивные поля
Более крупные ядра (5×5 и 7×7) обеспечивают больше охват контекста, что полезно для объектов, занимающих большее пространство.
Комбинация 1×1 + 3×3 эффективно увеличивает глубину и нелинейность без большого роста параметров.

Визуализация активаций первого слоя
3x3 активации более локальные, детализированные.
7x7 более размазанные, но схватывают крупные шаблоны.
1x1+3x3 — умеренный компромисс между локальностью и обобщением.

## 2.2 Влияние глубины CNN (15 баллов)

Глубина 2: неглубокая сеть.
Глубина 4: умеренно глубокая сеть.
Глубина 6: глубокая сеть.
Residual CNN: глубокая сеть с остаточными связями.

Сравнение точности и потерь
| Архитектура | Test Accuracy | Test Loss |
| ------------ | ------------- | ---------- |
| depth=2 | \~0.64 | \~1.03 |
| depth=4 | \~0.77 | \~0.68 |
| depth=6 | \~0.80 | \~0.59 |
| Residual CNN | \~0.72 | \~0.81 |

С увеличением глубины улучшалась точность и снижались потери, до определенного предела.
Residual CNN справилась лучше, чем простая глубина 2, но уступила глубине 6 по точности.

Анализ градиентов
На графиках видно, что глубокая сеть без Residual блоков справляется лучше.
Однако остаточные связи обеспечивают более стабильное обучение, особенно при увеличении числа слоев.

Глубина 6 — наиболее богатая детализация признаков.
Residual — активации хорошо передают границы и структуру, что полезно для обобщения.

## 3.1 Реализация кастомных слоев (15 баллов)

Были реализованы и протестированы следующие компоненты:

Кастомный сверточный слой с дополнительной логикой
Реализация включала модифицированный Conv2D, добавляющий learnable-маску после свертки.
Поведение слоя было проверено через сравнение с обычным nn.Conv2d.
В обучении наблюдалась незначительная прибавка в качестве: финальная test accuracy ≈ 0.715, против 0.70 для базовой модели.
На графике видно постепенное уменьшение train/test loss, без переобучения.

Attention-механизм для CNN
Внедрен spatial attention механизм, усиливающий важные регионы карты признаков.
Attention позволил модели обучаться быстрее и эффективнее: на 5-й эпохе accuracy обогнала базовую CNN.
Final test accuracy достигала 0.75+, при меньшем test loss — отражено на графиках.

Кастомная функция активации
Использовалась модифицированная ReLU с learnable порогом (например, PReLU).
Улучшения не столь значительны, но качество чуть выше стандартной ReLU.
Тренд test loss стабилен и ниже, чем при обычной активации.

Кастомный pooling слой
Реализован soft-pooling как дифференцируемая альтернатива MaxPool.
Качество сопоставимо с обычным pooling, но градиенты стабильнее, особенно на старте обучения.
Это хорошо видно по графику — плавная кривая без резких скачков loss/accuracy.

## 3.2 Эксперименты с Residual-блоками (15 баллов)

В рамках этого этапа были реализованы три типа Residual блоков и проведено их сравнение.

Basic Residual Block
Стандартный блок: два Conv + BN + ReLU и skip-связь.
Работал стабильно, без градиентных проблем.
Final test accuracy: ~0.71, loss: ~0.9 (см. левый график resblocks_comparison.png).

Bottleneck Block
Содержал 1×1 → 3×3 → 1×1 свертки для уменьшения числа параметров.
Accuracy: ~0.56, заметно ниже, вероятно из-за ограничения пропускной способности.
Loss остается высоким на протяжении всех эпох.

Wide Residual Block
Увеличено число каналов в Conv слоях без увеличения глубины.
Лучшая производительность из всех:
Final test accuracy: ~0.75+
Test loss: наименьший среди всех трех моделей
Отличный компромисс между глубиной и пропускной способностью.

Сравнение производительности
Wide > Basic > Bottleneck по метрикам точности и устойчивости к переобучению.
Все модели устойчивы к vanishing gradient благодаря skip-связям.
